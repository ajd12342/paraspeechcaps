{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from parler_tts import ParlerTTSForConditionalGeneration\n",
    "from transformers import AutoTokenizer, pipeline, WhisperForConditionalGeneration, WhisperTokenizer, WhisperTokenizerFast\n",
    "import soundfile as sf\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/local/anujd/data/anujd/conda_envs/paraspeechcaps/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "Config of the text_encoder: <class 'transformers.models.t5.modeling_t5.T5EncoderModel'> is overwritten by shared text_encoder config: T5Config {\n",
      "  \"_name_or_path\": \"google/flan-t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2816,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "Config of the audio_encoder: <class 'parler_tts.dac_wrapper.modeling_dac.DACModel'> is overwritten by shared audio_encoder config: DACConfig {\n",
      "  \"_name_or_path\": \"parler-tts/dac_44khZ_8kbps\",\n",
      "  \"architectures\": [\n",
      "    \"DACModel\"\n",
      "  ],\n",
      "  \"codebook_size\": 1024,\n",
      "  \"frame_rate\": 86,\n",
      "  \"latent_dim\": 1024,\n",
      "  \"model_bitrate\": 8,\n",
      "  \"model_type\": \"dac_on_the_hub\",\n",
      "  \"num_codebooks\": 9,\n",
      "  \"sampling_rate\": 44100,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.1\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'parler_tts.modeling_parler_tts.ParlerTTSForCausalLM'> is overwritten by shared decoder config: ParlerTTSDecoderConfig {\n",
      "  \"_name_or_path\": \"/fsx/yoach/tmp/artefacts/parler-tts-mini/decoder\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"ParlerTTSForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1025,\n",
      "  \"codebook_weights\": null,\n",
      "  \"cross_attention_implementation_strategy\": null,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 1024,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_factor\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"parler_tts_decoder\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codebooks\": 9,\n",
      "  \"num_cross_attention_key_value_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pad_token_id\": 1024,\n",
      "  \"rope_embeddings\": false,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"scale_embedding\": false,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_fused_lm_heads\": false,\n",
      "  \"vocab_size\": 1088\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"ajd12342/parler-tts-mini-v1-paraspeechcaps\" # Replace with \"ajd12342/parler-tts-mini-v1-paraspeechcaps-only-base\" for the model trained only on the PSC-Base dataset\n",
    "model = ParlerTTSForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "description_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "transcription_tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_description = \"In a clear environment, a male voice speaks with a sad tone.\".replace('\\n', ' ').rstrip()\n",
    "input_transcription = \"Was that your landlord?\".replace('\\n', ' ').rstrip()\n",
    "\n",
    "input_description_tokenized = description_tokenizer(input_description, return_tensors=\"pt\").to(model.device)\n",
    "input_transcription_tokenized = transcription_tokenizer(input_transcription, return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "generation = model.generate(input_ids=input_description_tokenized.input_ids, prompt_input_ids=input_transcription_tokenized.input_ids)\n",
    "\n",
    "audio_arr = generation.cpu().numpy().squeeze()\n",
    "sf.write(\"output_simple.wav\", audio_arr, model.config.sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference with classifier-free guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "guidance_scale = 1.5\n",
    "generation = model.generate(input_ids=input_description_tokenized.input_ids, prompt_input_ids=input_transcription_tokenized.input_ids, guidance_scale=guidance_scale)\n",
    "\n",
    "audio_arr = generation.cpu().numpy().squeeze()\n",
    "sf.write(\"output_with_cfg.wav\", audio_arr, model.config.sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference with classifier-free guidance and ASR-based resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from https://github.com/huggingface/parler-tts/blob/d108732cd57788ec86bc857d99a6cabd66663d68/training/eval.py\n",
    "asr_model_name_or_path = 'distil-whisper/distil-large-v2'\n",
    "asr_pipeline = pipeline(model=asr_model_name_or_path, device=device, chunk_length_s=25.0)\n",
    "\n",
    "def wer(asr_pipeline, prompt, audio, sampling_rate):\n",
    "    \"\"\"\n",
    "    Calculate Word Error Rate (WER) for a single audio sample against a reference text.\n",
    "    Args:\n",
    "        asr_pipeline: Huggingface ASR pipeline\n",
    "        prompt: Reference text string\n",
    "        audio: Audio array\n",
    "        sampling_rate: Audio sampling rate\n",
    "    \n",
    "    Returns:\n",
    "        float: Word Error Rate as a percentage\n",
    "    \"\"\"\n",
    "    # Load WER metric\n",
    "    metric = evaluate.load(\"wer\")\n",
    "\n",
    "    # Handle Whisper's return_language parameter\n",
    "    return_language = None\n",
    "    if isinstance(asr_pipeline.model, WhisperForConditionalGeneration):\n",
    "        return_language = True\n",
    "\n",
    "    # Transcribe audio\n",
    "    transcription = asr_pipeline(\n",
    "        {\"raw\": audio, \"sampling_rate\": sampling_rate},\n",
    "        return_language=return_language,\n",
    "    )\n",
    "\n",
    "    # Get appropriate normalizer\n",
    "    if isinstance(asr_pipeline.tokenizer, (WhisperTokenizer, WhisperTokenizerFast)):\n",
    "        tokenizer = asr_pipeline.tokenizer\n",
    "    else:\n",
    "        tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-large-v3\")\n",
    "\n",
    "    english_normalizer = tokenizer.normalize\n",
    "    basic_normalizer = tokenizer.basic_normalize\n",
    "\n",
    "    # Choose normalizer based on detected language\n",
    "    normalizer = (\n",
    "        english_normalizer\n",
    "        if isinstance(transcription.get(\"chunks\", None), list) \n",
    "        and transcription[\"chunks\"][0].get(\"language\", None) == \"english\"\n",
    "        else basic_normalizer\n",
    "    )\n",
    "\n",
    "    # Calculate WER\n",
    "    norm_pred = normalizer(transcription[\"text\"])\n",
    "    norm_ref = normalizer(prompt)\n",
    "    \n",
    "    return 100 * metric.compute(predictions=[norm_pred], references=[norm_ref])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/local/anujd/data/anujd/conda_envs/paraspeechcaps/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:509: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "You have passed task=transcribe, but also have set `forced_decoder_ids` to [[1, 50259], [2, 50359], [3, 50363]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of task=transcribe.\n"
     ]
    }
   ],
   "source": [
    "num_retries = 3\n",
    "wer_threshold = 20\n",
    "generated_audios = []\n",
    "word_errors = []\n",
    "for i in range(num_retries):\n",
    "    generation = model.generate(input_ids=input_description_tokenized.input_ids, prompt_input_ids=input_transcription_tokenized.input_ids, guidance_scale=guidance_scale)\n",
    "    audio_arr = generation.cpu().numpy().squeeze()\n",
    "\n",
    "    word_error = wer(asr_pipeline, input_transcription, audio_arr, model.config.sampling_rate)\n",
    "\n",
    "    if word_error < wer_threshold:\n",
    "        break\n",
    "    generated_audios.append(audio_arr)\n",
    "    word_errors.append(word_error)\n",
    "else:\n",
    "    # Pick the audio with the lowest WER\n",
    "    audio_arr = generated_audios[word_errors.index(min(word_errors))]\n",
    "\n",
    "sf.write(\"output_with_asr_resampling_and_cfg.wav\", audio_arr, model.config.sampling_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paraspeechcaps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
