{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from parler_tts import ParlerTTSForConditionalGeneration\n",
    "from transformers import AutoTokenizer, pipeline, WhisperForConditionalGeneration, WhisperTokenizer, WhisperTokenizerFast\n",
    "import soundfile as sf\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"ajd12342/parler-tts-mini-v1-paraspeechcaps\" # Replace with \"ajd12342/parler-tts-mini-v1-paraspeechcaps-only-base\" for the model trained only on the PSC-Base dataset\n",
    "model = ParlerTTSForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "description_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "transcription_tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying the input style prompt and text to be spoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_description = \"In a clear environment, a male voice speaks with a sad tone.\".replace('\\n', ' ').rstrip()\n",
    "input_transcription = \"Was that your landlord?\".replace('\\n', ' ').rstrip()\n",
    "\n",
    "input_description_tokenized = description_tokenizer(input_description, return_tensors=\"pt\").to(model.device)\n",
    "input_transcription_tokenized = transcription_tokenizer(input_transcription, return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation = model.generate(input_ids=input_description_tokenized.input_ids, prompt_input_ids=input_transcription_tokenized.input_ids)\n",
    "\n",
    "audio_arr = generation.cpu().numpy().squeeze()\n",
    "sf.write(\"output_simple.wav\", audio_arr, model.config.sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference with classifier-free guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guidance_scale = 1.5\n",
    "generation = model.generate(input_ids=input_description_tokenized.input_ids, prompt_input_ids=input_transcription_tokenized.input_ids, guidance_scale=guidance_scale)\n",
    "\n",
    "audio_arr = generation.cpu().numpy().squeeze()\n",
    "sf.write(\"output_with_cfg.wav\", audio_arr, model.config.sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference with classifier-free guidance and ASR-based resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from https://github.com/huggingface/parler-tts/blob/d108732cd57788ec86bc857d99a6cabd66663d68/training/eval.py\n",
    "asr_model_name_or_path = 'distil-whisper/distil-large-v2'\n",
    "asr_pipeline = pipeline(model=asr_model_name_or_path, device=device, chunk_length_s=25.0)\n",
    "\n",
    "def wer(asr_pipeline, prompt, audio, sampling_rate):\n",
    "    \"\"\"\n",
    "    Calculate Word Error Rate (WER) for a single audio sample against a reference text.\n",
    "    Args:\n",
    "        asr_pipeline: Huggingface ASR pipeline\n",
    "        prompt: Reference text string\n",
    "        audio: Audio array\n",
    "        sampling_rate: Audio sampling rate\n",
    "    \n",
    "    Returns:\n",
    "        float: Word Error Rate as a percentage\n",
    "    \"\"\"\n",
    "    # Load WER metric\n",
    "    metric = evaluate.load(\"wer\")\n",
    "\n",
    "    # Handle Whisper's return_language parameter\n",
    "    return_language = None\n",
    "    if isinstance(asr_pipeline.model, WhisperForConditionalGeneration):\n",
    "        return_language = True\n",
    "\n",
    "    # Transcribe audio\n",
    "    transcription = asr_pipeline(\n",
    "        {\"raw\": audio, \"sampling_rate\": sampling_rate},\n",
    "        return_language=return_language,\n",
    "    )\n",
    "\n",
    "    # Get appropriate normalizer\n",
    "    if isinstance(asr_pipeline.tokenizer, (WhisperTokenizer, WhisperTokenizerFast)):\n",
    "        tokenizer = asr_pipeline.tokenizer\n",
    "    else:\n",
    "        tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-large-v3\")\n",
    "\n",
    "    english_normalizer = tokenizer.normalize\n",
    "    basic_normalizer = tokenizer.basic_normalize\n",
    "\n",
    "    # Choose normalizer based on detected language\n",
    "    normalizer = (\n",
    "        english_normalizer\n",
    "        if isinstance(transcription.get(\"chunks\", None), list) \n",
    "        and transcription[\"chunks\"][0].get(\"language\", None) == \"english\"\n",
    "        else basic_normalizer\n",
    "    )\n",
    "\n",
    "    # Calculate WER\n",
    "    norm_pred = normalizer(transcription[\"text\"])\n",
    "    norm_ref = normalizer(prompt)\n",
    "    \n",
    "    return 100 * metric.compute(predictions=[norm_pred], references=[norm_ref])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_retries = 3\n",
    "wer_threshold = 20\n",
    "generated_audios = []\n",
    "word_errors = []\n",
    "for i in range(num_retries):\n",
    "    generation = model.generate(input_ids=input_description_tokenized.input_ids, prompt_input_ids=input_transcription_tokenized.input_ids, guidance_scale=guidance_scale)\n",
    "    audio_arr = generation.cpu().numpy().squeeze()\n",
    "\n",
    "    word_error = wer(asr_pipeline, input_transcription, audio_arr, model.config.sampling_rate)\n",
    "\n",
    "    if word_error < wer_threshold:\n",
    "        break\n",
    "    generated_audios.append(audio_arr)\n",
    "    word_errors.append(word_error)\n",
    "else:\n",
    "    # Pick the audio with the lowest WER\n",
    "    audio_arr = generated_audios[word_errors.index(min(word_errors))]\n",
    "\n",
    "sf.write(\"output_with_asr_resampling_and_cfg.wav\", audio_arr, model.config.sampling_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paraspeechcaps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
